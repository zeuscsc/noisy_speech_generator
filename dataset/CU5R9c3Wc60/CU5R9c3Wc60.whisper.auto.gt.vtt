 WEBVTT

00:00.000 --> 00:05.020
Hi,I'm Selva Prabhakaran.In this one,let's understand clearly the Bayesian optimization

00:05.020 --> 00:09.920
method for hyper parameter tuning.Now,you know,hyper parameter tuning is a key step in

00:09.920 --> 00:13.800
machine learning model building,but it can be quite time taking.So we will understand

00:13.800 --> 00:19.760
everything about Bayesian optimization specifically.Okay.But to be clear,let's quickly revise

00:19.760 --> 00:23.540
what is hyper parameter tuning in the first place so that everything will make sense and

00:23.540 --> 00:28.880
fall in place.This is how machine learning model building happens.Say you have the features and

00:28.880 --> 00:33.220
the variable that you want to predict X and Y respectively.This you have in your data,

00:33.380 --> 00:38.080
the data is split into training data set on which the model will be actually trained.Then you will

00:38.080 --> 00:42.900
do the prediction on validation data set,which has not been shown to the model during training,

00:43.100 --> 00:49.520
right?So you will do the prediction.So X and Y from the train is passed to the ML model.This is

00:49.520 --> 00:55.640
the training process.Okay.You get the predictions on the validation data set,the Y hat.Then you

00:55.640 --> 01:00.640
compare the Y hat against the actual Y from the validation data set.From that you compute the

01:00.640 --> 01:06.100
error,which is called the validation error,right?We want this error to be as low as possible.In

01:06.100 --> 01:12.340
order to achieve that,one of the techniques is we try to see how the model is performing.That is

01:12.340 --> 01:19.140
the error.Performance is lower the error,better the performances.Now we try to adjust various

01:19.140 --> 01:25.220
combinations of the hyper parameters of whichever ML model that we are dealing with.For example,

01:25.220 --> 01:29.840
if we are dealing with say a random forest algorithm,the hyper parameter could be something

01:29.840 --> 01:36.720
like the number of estimators and say the max depth,right?And could be other parameters also.

01:37.580 --> 01:43.780
Likewise,if you use say support vector missions,it could be what kernel you are using,what kernel

01:43.780 --> 01:49.640
you are using,what the value of the C parameters,things like that,right?So for a specific value

01:49.640 --> 01:55.280
of these hyper parameters,the error might be lowest.This is what hyper parameter tuning is

01:55.280 --> 02:00.560
all about.Now the process of hyper parameter tuning itself is done using the most common method is using

02:00.560 --> 02:05.240
what is called the grid search.For example,you have two hyper parameters,hyper parameter one and

02:05.240 --> 02:11.440
two,and these are the possible values of hyper parameters one and two on the Y axis,right?Now we want

02:11.440 --> 02:17.200
to see for various combinations of these hyper parameters,what is the model error is.Either the

02:17.200 --> 02:22.580
error can be the lowest or you compute the accuracy of the model and try to reach the maximum accuracy

02:22.580 --> 02:29.040
whichever way that you want to do it.So we train the models for all these combinations of both of these

02:29.040 --> 02:36.240
parameters and see which is the lowest error,let's say,okay?This is grid search approach.Whereas random

02:36.240 --> 02:42.560
search approach is slightly better than what grid search approach is,that is,we take random values

02:42.560 --> 02:47.520
of hyper parameters,both one and two randomly we pick.Here they are equally spaced usually typically

02:47.520 --> 02:52.720
equally play equally spaced or discrete values of hyper parameters.We choose specific values of hyper

02:52.720 --> 02:58.160
parameter two also.So it is always a combination of this and this.Whereas here,since we are randomly

02:58.160 --> 03:05.200
picking it,these points can be anywhere.This may not be in a grid.Because these points are can is

03:05.200 --> 03:12.640
randomly chosen between hyper parameters one and two.We get the model here gets to see various

03:12.640 --> 03:17.760
different values of the hyper parameters,right?So here,let's say the model got to see the

03:17.760 --> 03:23.120
hyper parameter value to only four different unique values.But see here,you can see the model has seen

03:23.120 --> 03:29.120
this value here,this value here,for this parameter,this value here,for this parameter,this value,

03:29.120 --> 03:33.680
so on and so forth,right?So the model gets to see multiple different values of hyper parameter,

03:33.680 --> 03:39.760
both one and two,both two and one here,right?So that is the advantage of random search.Okay,

03:39.760 --> 03:45.600
the searches,more,more variety will be there.However,the problem is,both of these approaches

03:45.600 --> 03:52.080
are time consuming.In random search,we will never ever get to probably know the lowest value of your

03:52.080 --> 03:56.400
hyper parameters.I mean,we have the combination of hyper parameters that is going to give you the

03:56.400 --> 04:01.760
lowest value of the errors.Also,the search space when we deal especially with grid search might be

04:01.760 --> 04:05.680
limited.For example,we are searching for these three values of hyper parameter one,

04:05.680 --> 04:10.160
what of the optimal value lies outside these values,right?That is also a limitation when we

04:10.160 --> 04:15.360
deal with both of these approaches.So these are drawbacks,but the biggest drawback with both of

04:15.360 --> 04:21.440
these approaches is this.Now,let's say here the red points and the green points are the points that

04:21.440 --> 04:27.440
you have hyper parameter one and hyper parameter hyper two hyper parameter two.Now these points,

04:27.440 --> 04:33.280
these reds and these greens are the points that the model has already trained on.The whites are

04:33.280 --> 04:38.000
yet to be trained.Now we know that the models performing good in this region and it's not so

04:38.000 --> 04:44.080
good in this region.Now with this information,this information is not used both of in both of grid

04:44.080 --> 04:49.680
search as well as random search.But if this is the information given to you,what would be the value

04:49.680 --> 04:55.120
or what would be the value that you would try for your next iteration?We are more likely to try around

04:55.120 --> 04:59.360
this area,isn't it?Because this is green,this is doing well,right?Because this area is doing

04:59.360 --> 05:04.640
well,we will try and find out for this particular region for the best possible parameter combination.

05:04.640 --> 05:09.680
But we don't take that information account when into account when we are using grid search as well

05:09.680 --> 05:16.960
as random search.So this is exactly what Bayesian optimization is all about.It tries to make use

05:16.960 --> 05:23.120
of the prior information to make the subsequent searches so that we will be able to find the optimal point

05:23.760 --> 05:30.080
quite fast.So this is the fundamental idea behind Bayesian optimization.But how does this work?

05:30.080 --> 05:37.600
So what we are concerned is about finding the probability of getting a specific model score,

05:37.600 --> 05:42.960
getting a specific model score.This could be model score.This could be a cost function,

05:42.960 --> 05:48.640
cost function or sorry,the loss function,loss function like error,some type of an error or

05:48.640 --> 05:53.520
it could be accuracy,right?Accuracy means we want to maximize it.Loss function means we want to

05:53.520 --> 05:57.600
minimize it.Error means we want to minimize it,right?So what is the probability of getting a particular

05:57.600 --> 06:04.400
model score given a particular configuration of the hyper parameters?So this is ultimately what we want

06:04.400 --> 06:12.400
to find out.But we want to find this.We want to find the configuration that gives the maximum or minimum

06:12.960 --> 06:18.000
depending on the objective,giving the what is the configuration that gives the maximum model score.

06:18.000 --> 06:21.280
But at the same time,we don't have the liberty of trying out

06:22.160 --> 06:28.080
all of these iterations,all of these configurations possible,right?The number of iterations or the

06:28.080 --> 06:32.880
number of iterations means training here.Each training process is what we refer to as iteration.

06:32.880 --> 06:37.040
The number of iterations is something that we want to reduce.So we want to achieve this

06:37.040 --> 06:46.160
objective with minimum number of iterations.How do we do this?This essentially boils down into four

06:46.160 --> 06:51.680
steps.First,we need a surrogate model.Surrogate model is nothing but the model where we predict the

06:51.680 --> 06:56.960
model score or the objective function,whatever we want to call it.So predict the objective function

06:56.960 --> 07:02.720
given a particular configuration of hyper parameters,right?So we will have a surrogate

07:02.720 --> 07:08.400
model.At the same time,we cannot estimate this for all possible combinations or all possible

07:08.400 --> 07:13.760
configurations,right?We want to minimize the number of iterations.So we need an acquisition

07:13.760 --> 07:18.960
function also.So this acquisition function will guide the search.Guide the search means if you

07:18.960 --> 07:23.680
are familiar with the concept of reinforcement learning,if you are familiar with reinforcement

07:24.480 --> 07:28.560
learning with the idea behind say multi-arm bandits and all this is quite similar to that

07:29.200 --> 07:35.280
where this acquisition function will have the opportunity to explore either explore or exploit.

07:35.280 --> 07:40.000
Exploit means what it means is basically we know that these regions are doing well.If we know that

07:40.000 --> 07:45.360
these regions are doing well,we will try to for the next iteration when it goes for an exploit option,

07:45.360 --> 07:49.920
it will try to choose data points around this region.Okay,so it will go for data points in this

07:49.920 --> 07:56.320
in this particular zone.That would be exploit.Explore would be it will try to find out or try to test out

07:56.320 --> 08:00.640
data points that is farther away right randomly to choose some other point somewhere in the board

08:00.640 --> 08:06.160
and we'll try to explore how the model score turns out to be for a different configuration.So that's

08:06.160 --> 08:11.840
what explore is.So to do this,there are various techniques such as the expected improvement method

08:11.840 --> 08:19.440
or the upper confidence bound method you can use to use as the acquisition function.So this will

08:19.440 --> 08:26.960
essentially guide how the model or how the algorithm is searching for the next iterate next configuration

08:26.960 --> 08:32.240
of the hyper parameter.So once the next iteration or configuration is chosen,we will use again

08:32.240 --> 08:40.000
the surrogate model in this step to evaluate the newly chosen configuration,right?So we will evaluate it

08:40.000 --> 08:46.640
find the score again score or the objective function again and then use that newly found information

08:46.640 --> 08:53.840
and store it back into the surrogate model again.So this process that is steps two and three,

08:53.840 --> 08:59.840
these will go on until the optimization or objective function does not improve anymore or the number of

08:59.840 --> 09:05.200
iterations that the user has set has been achieved.So this is the whole idea behind it.If you want

09:05.200 --> 09:11.120
to understand simply rights,if you want to understand simply what surrogate function versus surrogate

09:11.120 --> 09:17.040
function versus a acquisition function does is simply say this is your grid.This is your hyper

09:17.040 --> 09:23.440
parameter,hyper parameter grid.Say you have the hyper parameter one and hyper parameter two on the y

09:23.440 --> 09:30.320
axis,all right?Now you have this grid.Think of this grid as a map,okay?This think of this grid as a map

09:30.320 --> 09:36.560
where only few data points we have sampled,okay?We want to trace the lines in this map.Say you,you have

09:36.560 --> 09:42.880
a particular country and you want to trace the lines and the lines where the lines are the lines

09:42.880 --> 09:49.120
are the points where your objective function,the objective function is highest.So higher the

09:49.120 --> 09:52.960
objective function,suppose if it's accuracy,we want to maximize it,right?Suppose your objective is

09:52.960 --> 10:00.960
increasing the accuracy.We want to go in a in a particular route that will reach us to the maximum.

10:00.960 --> 10:05.760
Suppose this is the point where it has the maximum highest accuracy starting somewhere randomly.

10:05.760 --> 10:11.840
We want to reach this point at the fastest rate,right?The acquisition function,the acquisition

10:11.840 --> 10:15.920
function is something that will,that will estimate the height of these points,okay?

10:15.920 --> 10:21.840
Think of this as a contour.You might have seen contour graphs and all where you have say red colors,

10:21.840 --> 10:26.320
dark red colors,wherever the point is very high.The,the value is very high heat maps kind of thing,

10:26.960 --> 10:32.400
right?And the milder the color becomes the,the lower the values are,okay?Think of it like this.

10:32.400 --> 10:37.920
So the acquisition point acquisition function will help you estimate the value of the objective

10:37.920 --> 10:44.800
function at any given point,right?Likewise,the,okay,sorry,the surrogate function.I'm talking about

10:44.800 --> 10:51.200
surrogate function here.Surrogate function will estimate this objective value and the,and the acquisition

10:51.200 --> 10:58.000
function acquisition function will decide what route to take in order to reach the optimal point.So this is the

10:58.000 --> 11:03.520
fundamental idea behind Bayesian optimization.Let's implement this in code.Okay.So we will see a couple

11:03.520 --> 11:11.360
of implementations of hyper parameter optimization.One is using the optuna package.The other is using

11:11.360 --> 11:18.480
GPyOpt.So both are good.We will see both in this one.So for optuna,you need to first install

11:18.480 --> 11:24.480
pip install optuna and scikit-learn.All right.So once that is done,I'll give you the link to the,

11:24.480 --> 11:29.520
this,this particular data set or this particular notebook in the description,please use that link

11:29.520 --> 11:35.920
to try to get this and try it out.First,we will import the packages optuna numpy pandas.

11:35.920 --> 11:40.960
And from scikit-learn,we are going to be using the breast cancer data set.And we will also use

11:40.960 --> 11:46.240
these also.All right.So basic steps.First,we will download the data,load the data set,which is

11:46.240 --> 11:52.320
the breast cancer data set.We get the X,the features and the Y form the use strain test split to form

11:52.320 --> 11:57.920
your X-train,X-validation,Y-train and Y-validation.Basic steps.Now,in order to do the

11:57.920 --> 12:02.560
hyper parameter optimization,we define the objective function.This is the key.This is

12:02.560 --> 12:07.280
the main part where you define what the hyper parameters are.So you are passing it into a

12:07.280 --> 12:12.480
dictionary.Okay.Hyper parameters and what could be the potential values each of these parameters

12:12.480 --> 12:17.600
could take.Okay.Depending on what the nature of the values are here,these are integers.So trial

12:17.600 --> 12:21.520
dot suggestion.Okay.This trial is the object that we are passing to the objective itself.

12:21.520 --> 12:26.720
Then we will train the,train the random forest classifier,fit it.That is,this is the training

12:26.720 --> 12:32.160
part itself.Then get the predicted values and compute the accuracy score.And we are returning

12:32.160 --> 12:38.320
the accuracy score.This is the objective function.Now,once that is done,we are ready to create a study

12:38.880 --> 12:44.400
that we are using the optuna.This is present as a method inside optuna object itself.And since we

12:44.400 --> 12:49.520
are using accuracy,we are setting the direction as maximize.Okay.So the study is created.

12:49.520 --> 12:54.080
They've called study dot optimize.The number of trials we are setting as 100.You can increase

12:54.080 --> 12:59.360
this number if you have more computation power and patience.All right.So this is the,we are printing

12:59.360 --> 13:05.120
out the,the number of trials it took,the best parameters and the trial value itself.So I have

13:05.120 --> 13:12.720
already run this code and you can see it has run for about 100 different iterations.And at the end,

13:12.720 --> 13:19.440
you get an accuracy of 0.9649.Okay.So this is the best performance.So once the model is,once the

13:19.440 --> 13:24.880
training is done,the search is done,we use the best parameters.So from study,get the best trial

13:24.880 --> 13:29.920
and the parameters from that trail and rebuild this model.Okay.We have to retrain this model using the

13:29.920 --> 13:35.280
best parameters and then you can do the prediction also.So this is based on the optuna packaging.

13:35.280 --> 13:40.480
Now,similarly,if you slightly go down in this notebook,we have the grid search CV as well

13:40.480 --> 13:48.000
as the random search.After this,we will implement using GPyOpt.So this is another package where you

13:48.000 --> 13:55.120
can use implement Bayesian optimization.We're first installing this now import GPyOpt and this is

13:55.120 --> 14:01.040
implemented on the iris data set,same random forest classifier.Now we load the same steps.We load

14:01.040 --> 14:07.920
the iris,the x and the y,then define the objective function again,again,same,same stuff.So we are

14:07.920 --> 14:12.480
defining the parameters.These parameters,instead of having it as a dictionary,we are estimating,

14:12.480 --> 14:17.520
we are creating separate objects for the parameters,then pass them into the random forest classifier.

14:17.520 --> 14:22.640
So these values are going directly inside.All right.Then we are computing the score.And since we

14:22.640 --> 14:28.400
want to minimize,it is always going to be minimized here.We are getting the score and adding a negative

14:28.400 --> 14:33.600
to it.So now it's going to be,even though this is an accuracy,we are going to minimize here because

14:33.600 --> 14:38.880
this is having a negative sign.So that is done.We set the bounds for the various,various hyper

14:38.880 --> 14:46.400
parameters.We are defining the bounds and call GPyOpt dot methods Bayesian optimization.We pass the

14:46.400 --> 14:53.280
objective function bounds and acquisition function type equal to ei expected improvement.You could also

14:53.280 --> 14:58.160
try the other options,especially the UCB option,which could also work well.Now,once that is done,

14:58.400 --> 15:03.360
we run the optimization,then get the best parameters,convert to integer values,and then

15:03.360 --> 15:08.240
you can get the best parameter values.Now,once the best parameters values are estimated,

15:08.960 --> 15:14.480
we need to sort of rebuild the model again.That is the random forest classifier with the best

15:14.480 --> 15:19.840
parameters that we have found out.After building that,you can do the train test split,fit the model

15:19.840 --> 15:24.640
and get the best predictions.So this is the code quite straightforward.Use the link in the

15:24.640 --> 15:26.640
description to try this out on your own.

