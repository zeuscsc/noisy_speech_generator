WEBVTT

00:10:58.000 --> 00:11:03.520
fundamental idea behind Bayesian optimization. Let's implement this in code. Okay. So we will see a couple

00:11:03.520 --> 00:11:11.360
of implementations of hyper parameter optimization. One is using the optuna package. The other is using

00:11:11.360 --> 00:11:18.480
g pi opt. So both are good. We will see both in this one. So for optuna, you need to first install

00:11:18.480 --> 00:11:24.480
pip install optuna and scikit-learn. All right. So once that is done, I'll give you the link to the,

00:11:24.480 --> 00:11:29.520
this, this particular data set or this particular notebook in the description, please use that link

00:11:29.520 --> 00:11:35.920
to try to get this and try it out. First, we will import the packages optuna numpy pandas.

00:11:35.920 --> 00:11:40.960
And from scikit-learn, we are going to be using the breast cancer data set. And we will also use

00:11:40.960 --> 00:11:46.240
these also. All right. So basic steps. First, we will download the data, load the data set, which is

00:11:46.240 --> 00:11:52.320
the breast cancer data set. We get the X, the features and the Y form the use strain test split to form

00:11:52.320 --> 00:11:57.920
your X-train, X-validation, Y-train and Y-validation. Basic steps. Now, in order to do the

00:11:57.920 --> 00:12:02.560
hyper parameter optimization, we define the objective function. This is the key. This is
