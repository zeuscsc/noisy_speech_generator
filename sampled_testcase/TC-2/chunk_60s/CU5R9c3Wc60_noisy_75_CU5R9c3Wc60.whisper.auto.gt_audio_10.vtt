WEBVTT

00:00:00.000 --> 00:00:05.440
right? And the milder the color becomes the, the lower the values are, okay? Think of it like this.

00:00:05.440 --> 00:00:10.960
So the acquisition point acquisition function will help you estimate the value of the objective

00:00:10.960 --> 00:00:17.840
function at any given point, right? Likewise, the, okay, sorry, the surrogate function. I'm talking about

00:00:17.840 --> 00:00:24.240
surrogate function here. Surrogate function will estimate this objective value and the, and the acquisition

00:00:24.240 --> 00:00:31.040
function acquisition function will decide what route to take in order to reach the optimal point. So this is the

00:00:31.040 --> 00:00:36.560
fundamental idea behind Bayesian optimization. Let's implement this in code. Okay. So we will see a couple

00:00:36.560 --> 00:00:44.400
of implementations of hyper parameter optimization. One is using the optuna package. The other is using

00:00:44.400 --> 00:00:51.520
GPyOpt. So both are good. We will see both in this one. So for optuna, you need to first install

00:00:51.520 --> 00:00:57.520
pip install optuna and scikit-learn. All right. So once that is done, I'll give you the link to the,

00:00:57.520 --> 00:01:02.560
this, this particular data set or this particular notebook in the description, please use that link
